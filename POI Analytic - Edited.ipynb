{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key backend.qt4 in file /etc/matplotlib/matplotlibrc, line 43 ('backend.qt4 : PyQt4        # PyQt4 | PySide')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.54.0.7:20050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0.cloudera2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn-client</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>poi-reseller</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbb48600588>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# pyspark\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import types, functions as F\n",
    "\n",
    "# random seed for reproducibility\n",
    "RANDOM_SEED = 15\n",
    "\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from cycler import cycler\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "app_name = 'poi-reseller'\n",
    "conf = (SparkConf()\n",
    "        .setMaster(\"yarn-client\")\n",
    "        .setAppName(app_name)\n",
    "        .set(\"spark.executor.memory\", \"8g\")\n",
    "        .set(\"spark.executor.instances\", \"1\")\n",
    "        .set(\"spark.executor.cores\", \"4\")\n",
    "        .set(\"spark.driver.memory\", \"8g\")\n",
    "        .set(\"spark.yarn.queue\", \"root.hue_dmp\")\n",
    "        .set(\"spark.default.parallelism\", \"8\")\n",
    "        .set(\"spark.sql.shuffle.partitions\", \"1000\")\n",
    "        .set(\"spark.shuffle.service.enabled\", \"true\")\n",
    "        .set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "        .set(\"spark.yarn.driver.memoryOverhead\", \"4096\")\n",
    "        .set(\"spark.yarn.executor.memoryOverhead\", \"4096\")\n",
    "        .set(\"spark.kryoserializer.buffer.max\", \"1g\")\n",
    "        .set(\"spark.dynamicAllocation.minExecutors\", \"1\")\n",
    "        .set(\"spark.dynamicAllocation.maxExecutors\", \"20\")\n",
    "        .set(\"spark.driver.maxResultSize\", \"4g\")\n",
    "        .set(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "        .set(\"spark.hadoop.fs.permissions.umask-mode\", \"002\")\n",
    "        .set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "        .set(\"spark.dynamicAllocation.initialExecutors\", \"1\"))\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "df = spark.sql(\"\"\"select \n",
    "                        b.kabupaten,\n",
    "                        sum(b.total_trx) as total_trx,\n",
    "                        sum(b.fea_outlet_under_100m_to_leisure_and_outdoor) fea_outlet_under_100m_to_leisure_and_outdoor,\n",
    "                        sum(b.fea_outlet_under_100m_to_areas_and_buildings) fea_outlet_under_100m_to_areas_and_buildings,\n",
    "                        sum(b.fea_outlet_under_100m_to_shopping) fea_outlet_under_100m_to_shopping,\n",
    "                        sum(b.fea_outlet_under_100m_to_going_out_entertainment) fea_outlet_under_100m_to_going_out_entertainment,\n",
    "                        sum(b.fea_outlet_under_100m_to_sights_and_museums) fea_outlet_under_100m_to_sights_and_museums,\n",
    "                        sum(b.fea_outlet_under_100m_to_facilities) fea_outlet_under_100m_to_facilities,\n",
    "                        sum(b.fea_outlet_under_100m_to_accommodation) fea_outlet_under_100m_to_accommodation,\n",
    "                        sum(b.fea_outlet_under_100m_to_natural_and_geographical) fea_outlet_under_100m_to_natural_and_geographical,\n",
    "                        sum(b.fea_outlet_under_100m_to_business_and_services) fea_outlet_under_100m_to_business_and_services,\n",
    "                        sum(b.fea_outlet_under_100m_to_transport) fea_outlet_under_100m_to_transport,            \n",
    "                        sum(b.fea_outlet_under_100m_to_eat_and_drink) fea_outlet_under_100m_to_eat_and_drink\n",
    "                    from\n",
    "                        mck.ra_digipos_poi_202102 b \n",
    "                    where \n",
    "                        b.outlet_location_type == 'FISIK' and\n",
    "                     b.business_partner == 'N'\n",
    "                     group by 1\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df[['fea_outlet_under_100m_to_leisure_and_outdoor', \n",
    "           'fea_outlet_under_100m_to_areas_and_buildings',\n",
    "           'fea_outlet_under_100m_to_shopping',\n",
    "           'fea_outlet_under_100m_to_going_out_entertainment',\n",
    "           'fea_outlet_under_100m_to_sights_and_museums',\n",
    "           'fea_outlet_under_100m_to_facilities',\n",
    "           'fea_outlet_under_100m_to_accommodation',\n",
    "           'fea_outlet_under_100m_to_natural_and_geographical',\n",
    "           'fea_outlet_under_100m_to_business_and_services',\n",
    "           'fea_outlet_under_100m_to_transport',\n",
    "           'fea_outlet_under_100m_to_eat_and_drink'\n",
    "          ]].corrwith(df['total_trx']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X           = df.drop(['total_trx','kabupaten'], axis=1)\n",
    "y           = df[['total_trx']]\n",
    "y           = np.log(y['total_trx']+1)\n",
    "model       = xgb.XGBRegressor().fit(X, y)\n",
    "explainer   = shap.Explainer(model)\n",
    "shap_values = explainer(X)\n",
    "shap.plots.beeswarm(shap_values, max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import absolute\n",
    "model       = xgb.XGBRegressor().fit(X, y)\n",
    "cv          = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "scores      = cross_val_score(model, X, y, scoring='r2', cv=cv, n_jobs=-1)\n",
    "scores      = absolute(scores)\n",
    "print('Mean r2: %.1f (%.1f)' % (scores.mean(), scores.std()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.SCORERS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
