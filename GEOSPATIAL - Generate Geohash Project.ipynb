{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key backend.qt4 in file /etc/matplotlib/matplotlibrc, line 43 ('backend.qt4 : PyQt4        # PyQt4 | PySide')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from functools import reduce\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = 'nat-geohash'\n",
    "conf = (SparkConf()\n",
    "        .setMaster(\"yarn-client\")\n",
    "        .setAppName(app_name)\n",
    "        .set(\"spark.driver.maxResultSize\", \"10g\")\n",
    "        .set(\"spark.driver.memory\", \"16g\")\n",
    "        .set(\"spark.driver.memoryOverhead\", \"4096\")\n",
    "        .set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "        .set(\"spark.dynamicAllocation.initialExecutors\", \"1\")\n",
    "        .set(\"spark.dynamicAllocation.maxExecutors\", \"75\")\n",
    "        .set(\"spark.dynamicAllocation.minExecutors\", \"1\")\n",
    "        .set(\"spark.executor.cores\", \"8\")\n",
    "        .set(\"spark.executor.memory\", \"40G\")\n",
    "        .set(\"spark.hadoop.fs.permissions.umask-mode\", \"002\")\n",
    "        .set(\"spark.kryoserializer.buffer.max\", \"512m\")\n",
    "        .set(\"spark.shuffle.service.enabled\", \"true\")\n",
    "        .set(\"spark.sql.broadcastTimeout\", \"1000\")\n",
    "        .set(\"spark.sql.hive.convertMetastoreParquet\", \"false\")\n",
    "        .set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "        .set(\"spark.sql.shuffle.partitions\", \"1000\")\n",
    "        .set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "        .set(\"spark.yarn.driver.memoryOverhead\", \"4096\")\n",
    "        .set(\"spark.yarn.executor.memoryOverhead\", \"8G\")\n",
    "        .set(\"spark.yarn.maxAppAttempts\", \"2\")\n",
    "        .set(\"spark.yarn.queue\", \"root.hue_dmp\")\n",
    "        .set(\"yarn.nodemanager.vmem-check-enabled\", \"false\")\n",
    "        .set(\"hive.exec.dynamic.partition\", \"true\")\n",
    "        .set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "        .set(\"hive.exec.max.dynamic.partitions\", \"2048\")\n",
    "        )\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "site_id          object\n",
       "site_name        object\n",
       "long            float64\n",
       "lat             float64\n",
       "class            object\n",
       "kecamatan        object\n",
       "kabupaten        object\n",
       "region           object\n",
       "availability    float64\n",
       "cei_rating      float64\n",
       "nei_rating      float64\n",
       "subs_total      float64\n",
       "subs_4g         float64\n",
       "bandwidth       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/cdsw/dataset_alfan_202105281423.csv')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "site_id          object\n",
       "site_name        object\n",
       "long            float64\n",
       "lat             float64\n",
       "class            object\n",
       "kecamatan        object\n",
       "kabupaten        object\n",
       "region           object\n",
       "availability    float64\n",
       "cei_rating      float64\n",
       "nei_rating      float64\n",
       "subs_total      float64\n",
       "subs_4g         float64\n",
       "bandwidth       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.appName('pandasToSparkDF').getOrCreate()\n",
    "\n",
    "df_p = pd.read_csv('/home/cdsw/dataset_alfan_202105281423.csv')\n",
    "df_p.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "mySchema = StructType([ StructField(\"site_id\", StringType(), True)\\\n",
    "                       ,StructField(\"site_name\", StringType(), True)\\\n",
    "                       ,StructField(\"long\", DoubleType(), True)\\\n",
    "                       ,StructField(\"lat\", DoubleType(), True)\\\n",
    "                       ,StructField(\"class\", StringType(), True)\\\n",
    "                       ,StructField(\"kecamatan\", StringType(), True)\\\n",
    "                       ,StructField(\"kabupaten\", StringType(), True)\\\n",
    "                       ,StructField(\"availability\", StringType(), True)\\\n",
    "                       ,StructField(\"cei_rating\", StringType(), True)\\\n",
    "                       ,StructField(\"nei_rating\", StringType(), True)\\\n",
    "                       ,StructField(\"subs_total\", StringType(), True)\\\n",
    "                       ,StructField(\"subs_4g\", StringType(), True)\\\n",
    "                       ,StructField(\"bandwidth\", StringType(), True)\\\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(df_p,schema=mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_id</th>\n",
       "      <th>site_name</th>\n",
       "      <th>long</th>\n",
       "      <th>lat</th>\n",
       "      <th>class</th>\n",
       "      <th>kecamatan</th>\n",
       "      <th>kecamatan</th>\n",
       "      <th>kabupaten</th>\n",
       "      <th>availability</th>\n",
       "      <th>cei_rating</th>\n",
       "      <th>nei_rating</th>\n",
       "      <th>subs_total</th>\n",
       "      <th>subs_4g</th>\n",
       "      <th>bandwidth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ADL001</td>\n",
       "      <td>ADL001M41_Alebo</td>\n",
       "      <td>122.455860</td>\n",
       "      <td>-4.096810</td>\n",
       "      <td>Platinum</td>\n",
       "      <td>KONDA</td>\n",
       "      <td>KONAWE SELATAN</td>\n",
       "      <td>SULAWESI</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.722954261944301</td>\n",
       "      <td>3.3058374586206902</td>\n",
       "      <td>2836.0</td>\n",
       "      <td>1188.0</td>\n",
       "      <td>210.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ADL003</td>\n",
       "      <td>ADL003M41_Kasipute</td>\n",
       "      <td>122.039410</td>\n",
       "      <td>-4.766620</td>\n",
       "      <td>Platinum</td>\n",
       "      <td>RUMBIA</td>\n",
       "      <td>BOMBANA</td>\n",
       "      <td>SULAWESI</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.892454220014157</td>\n",
       "      <td>3.3011264310344823</td>\n",
       "      <td>2670.0</td>\n",
       "      <td>1425.0</td>\n",
       "      <td>330.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ADL005</td>\n",
       "      <td>ADL005M41_Tinanggea</td>\n",
       "      <td>122.226300</td>\n",
       "      <td>-4.458700</td>\n",
       "      <td>Platinum</td>\n",
       "      <td>TINANGGEA</td>\n",
       "      <td>KONAWE SELATAN</td>\n",
       "      <td>SULAWESI</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.71091903941682</td>\n",
       "      <td>3.3118165137931035</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>1366.0</td>\n",
       "      <td>280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ADL006</td>\n",
       "      <td>ADL006M41_Buapinang-DMT</td>\n",
       "      <td>121.581940</td>\n",
       "      <td>-4.772220</td>\n",
       "      <td>Gold</td>\n",
       "      <td>POLEANG</td>\n",
       "      <td>BOMBANA</td>\n",
       "      <td>SULAWESI</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.725821964580212</td>\n",
       "      <td>3.2454023137931034</td>\n",
       "      <td>2083.0</td>\n",
       "      <td>683.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ADL007</td>\n",
       "      <td>ADL007M41_Andolo</td>\n",
       "      <td>122.275120</td>\n",
       "      <td>-4.336910</td>\n",
       "      <td>Platinum</td>\n",
       "      <td>ANDOOLO</td>\n",
       "      <td>KONAWE SELATAN</td>\n",
       "      <td>SULAWESI</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.563447269577207</td>\n",
       "      <td>3.351412144827586</td>\n",
       "      <td>2333.0</td>\n",
       "      <td>977.0</td>\n",
       "      <td>290.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58423</td>\n",
       "      <td>YOG716</td>\n",
       "      <td>YOG716M41_PURBAYAN2-TBG</td>\n",
       "      <td>110.402750</td>\n",
       "      <td>-7.827980</td>\n",
       "      <td>Bronze</td>\n",
       "      <td>KOTAGEDE</td>\n",
       "      <td>KOTA YOGYAKARTA</td>\n",
       "      <td>CENTRAL JAVA</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.24227184580573</td>\n",
       "      <td>3.3267816310344824</td>\n",
       "      <td>546.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58424</td>\n",
       "      <td>YOG717</td>\n",
       "      <td>YOG717M41_PASARKARANGKAJEN-TBG</td>\n",
       "      <td>110.373118</td>\n",
       "      <td>-7.824756</td>\n",
       "      <td>Bronze</td>\n",
       "      <td>MERGANGSAN</td>\n",
       "      <td>KOTA YOGYAKARTA</td>\n",
       "      <td>CENTRAL JAVA</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.455395736724005</td>\n",
       "      <td>3.383850610344828</td>\n",
       "      <td>731.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58425</td>\n",
       "      <td>YOG719</td>\n",
       "      <td>YOG719M41_KLIRINGAN</td>\n",
       "      <td>110.371000</td>\n",
       "      <td>-7.789100</td>\n",
       "      <td>Bronze</td>\n",
       "      <td>GONDOKUSUMAN</td>\n",
       "      <td>KOTA YOGYAKARTA</td>\n",
       "      <td>CENTRAL JAVA</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.472529069553235</td>\n",
       "      <td>3.3613793275862065</td>\n",
       "      <td>580.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58426</td>\n",
       "      <td>YOG733</td>\n",
       "      <td>YOG733M41_UGMRELOK-TKM</td>\n",
       "      <td>110.374411</td>\n",
       "      <td>-7.767210</td>\n",
       "      <td>Gold</td>\n",
       "      <td>MLATI</td>\n",
       "      <td>SLEMAN</td>\n",
       "      <td>CENTRAL JAVA</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.3677981268213655</td>\n",
       "      <td>3.3399204310344826</td>\n",
       "      <td>959.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>285.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58427</td>\n",
       "      <td>YOG734</td>\n",
       "      <td>YOG734M41_SOROSUTAN2-TBG</td>\n",
       "      <td>110.386430</td>\n",
       "      <td>-7.830440</td>\n",
       "      <td>Bronze</td>\n",
       "      <td>UMBULHARJO</td>\n",
       "      <td>KOTA YOGYAKARTA</td>\n",
       "      <td>CENTRAL JAVA</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.352266883993355</td>\n",
       "      <td>3.4056321827586205</td>\n",
       "      <td>395.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58428 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      site_id                       site_name        long       lat     class  \\\n",
       "0      ADL001                 ADL001M41_Alebo  122.455860 -4.096810  Platinum   \n",
       "1      ADL003              ADL003M41_Kasipute  122.039410 -4.766620  Platinum   \n",
       "2      ADL005             ADL005M41_Tinanggea  122.226300 -4.458700  Platinum   \n",
       "3      ADL006         ADL006M41_Buapinang-DMT  121.581940 -4.772220      Gold   \n",
       "4      ADL007                ADL007M41_Andolo  122.275120 -4.336910  Platinum   \n",
       "...       ...                             ...         ...       ...       ...   \n",
       "58423  YOG716         YOG716M41_PURBAYAN2-TBG  110.402750 -7.827980    Bronze   \n",
       "58424  YOG717  YOG717M41_PASARKARANGKAJEN-TBG  110.373118 -7.824756    Bronze   \n",
       "58425  YOG719             YOG719M41_KLIRINGAN  110.371000 -7.789100    Bronze   \n",
       "58426  YOG733          YOG733M41_UGMRELOK-TKM  110.374411 -7.767210      Gold   \n",
       "58427  YOG734        YOG734M41_SOROSUTAN2-TBG  110.386430 -7.830440    Bronze   \n",
       "\n",
       "          kecamatan        kecamatan     kabupaten availability  \\\n",
       "0             KONDA   KONAWE SELATAN      SULAWESI        100.0   \n",
       "1            RUMBIA          BOMBANA      SULAWESI        100.0   \n",
       "2         TINANGGEA   KONAWE SELATAN      SULAWESI        100.0   \n",
       "3           POLEANG          BOMBANA      SULAWESI        100.0   \n",
       "4           ANDOOLO   KONAWE SELATAN      SULAWESI        100.0   \n",
       "...             ...              ...           ...          ...   \n",
       "58423      KOTAGEDE  KOTA YOGYAKARTA  CENTRAL JAVA        100.0   \n",
       "58424    MERGANGSAN  KOTA YOGYAKARTA  CENTRAL JAVA        100.0   \n",
       "58425  GONDOKUSUMAN  KOTA YOGYAKARTA  CENTRAL JAVA        100.0   \n",
       "58426         MLATI           SLEMAN  CENTRAL JAVA        100.0   \n",
       "58427    UMBULHARJO  KOTA YOGYAKARTA  CENTRAL JAVA        100.0   \n",
       "\n",
       "               cei_rating          nei_rating subs_total subs_4g bandwidth  \n",
       "0       3.722954261944301  3.3058374586206902     2836.0  1188.0     210.0  \n",
       "1       3.892454220014157  3.3011264310344823     2670.0  1425.0     330.0  \n",
       "2        3.71091903941682  3.3118165137931035     3873.0  1366.0     280.0  \n",
       "3       3.725821964580212  3.2454023137931034     2083.0   683.0     150.0  \n",
       "4       3.563447269577207   3.351412144827586     2333.0   977.0     290.0  \n",
       "...                   ...                 ...        ...     ...       ...  \n",
       "58423    4.24227184580573  3.3267816310344824      546.0   113.0      60.0  \n",
       "58424   4.455395736724005   3.383850610344828      731.0   216.0      90.0  \n",
       "58425   4.472529069553235  3.3613793275862065      580.0   194.0      90.0  \n",
       "58426  4.3677981268213655  3.3399204310344826      959.0   401.0     285.0  \n",
       "58427   4.352266883993355  3.4056321827586205      395.0    62.0      90.0  \n",
       "\n",
       "[58428 rows x 14 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Found duplicate column(s) in the table definition of `mck`.`sample_site_for_outlet`: `kecamatan`;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o108.sql.\n: org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the table definition of `mck`.`sample_site_for_outlet`: `kecamatan`;\n\tat org.apache.spark.sql.util.SchemaUtils$.checkColumnNameDuplication(SchemaUtils.scala:85)\n\tat org.apache.spark.sql.util.SchemaUtils$.checkSchemaColumnNameDuplication(SchemaUtils.scala:42)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableCreation.org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(rules.scala:236)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:207)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsDown(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperators(AnalysisHelper.scala:73)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:76)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:72)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\n\tat scala.collection.mutable.ArrayBuffer.foldLeft(ArrayBuffer.scala:48)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-0b9fff71571a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"create table mck.sample_site_for_outlet as select * from temp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Found duplicate column(s) in the table definition of `mck`.`sample_site_for_outlet`: `kecamatan`;'"
     ]
    }
   ],
   "source": [
    "spark.sql(\"create table mck.sample_site_for_outlet as select * from temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.54.0.8:20050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0.cloudera2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn-client</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>nat-geohash</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f498bf48278>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_name = 'nat-geohash'\n",
    "conf = (SparkConf()\n",
    "        .setMaster(\"yarn-client\")\n",
    "        .setAppName(app_name)\n",
    "        .set(\"spark.executor.memory\", \"8g\")\n",
    "        .set(\"spark.executor.instances\", \"1\")\n",
    "        .set(\"spark.executor.cores\", \"4\")\n",
    "        .set(\"spark.driver.memory\", \"8g\")\n",
    "        .set(\"spark.yarn.queue\", \"root.hue_dmp\")\n",
    "        .set(\"spark.default.parallelism\", \"8\")\n",
    "        .set(\"spark.sql.shuffle.partitions\", \"1000\")\n",
    "        .set(\"spark.shuffle.service.enabled\", \"true\")\n",
    "        .set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "        .set(\"spark.yarn.driver.memoryOverhead\", \"4096\")\n",
    "        .set(\"spark.yarn.executor.memoryOverhead\", \"4096\")\n",
    "        .set(\"spark.kryoserializer.buffer.max\", \"1g\")\n",
    "        .set(\"spark.dynamicAllocation.minExecutors\", \"1\")\n",
    "        .set(\"spark.dynamicAllocation.maxExecutors\", \"20\")\n",
    "        .set(\"spark.driver.maxResultSize\", \"4g\")\n",
    "        .set(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "        .set(\"spark.hadoop.fs.permissions.umask-mode\", \"002\")\n",
    "        .set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "        .set(\"spark.dynamicAllocation.initialExecutors\", \"1\"))\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.54.0.8:20050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0.cloudera2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn-client</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>nat-geohash</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f498bf48278>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Outlet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: hdfs://nameservice1/user/alfanwib/dataset_alfan_202105281423.csv;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o112.load.\n: org.apache.spark.sql.AnalysisException: Path does not exist: hdfs://nameservice1/user/alfanwib/dataset_alfan_202105281423.csv;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:558)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1835d77f43cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_site\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset_alfan_202105281423.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: hdfs://nameservice1/user/alfanwib/dataset_alfan_202105281423.csv;'"
     ]
    }
   ],
   "source": [
    "df_site = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dataset_alfan_202105281423.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlet = spark.read.table(\"mck.ra_outlet_dim_202102\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlet = df_outlet.select(\"outlet_id\",\"latitude\",\"longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o575.count.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange SinglePartition\n+- *(1) HashAggregate(keys=[], functions=[partial_count(1)], output=[count#122L])\n   +- Scan hive mck.ra_outlet_dim_202102 HiveTableRelation `mck`.`ra_outlet_dim_202102`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [outlet_id#65, created_at#66, outlet_name#67, kabupaten#68, longitude#69, latitude#70, outlet_type#71, regional#72, outlet_classification#73, kecamatan#74, kelurahan#75, cluster#76, branch#77, area#78, outlet_category#79, outlet_location_type#80, terakhir_dikunjungi#81, outlet_location_classification#82, rank#83, date#84, month#85, year#86, created_date#87, los#88]\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2830)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2829)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2829)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:745)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2359)\n\tat org.apache.spark.SparkContext.defaultMinPartitions(SparkContext.scala:2368)\n\tat org.apache.spark.sql.hive.HadoopTableReader.<init>(TableReader.scala:82)\n\tat org.apache.spark.sql.hive.execution.HiveTableScanExec.org$apache$spark$sql$hive$execution$HiveTableScanExec$$hadoopReader$lzycompute(HiveTableScanExec.scala:105)\n\tat org.apache.spark.sql.hive.execution.HiveTableScanExec.org$apache$spark$sql$hive$execution$HiveTableScanExec$$hadoopReader(HiveTableScanExec.scala:105)\n\tat org.apache.spark.sql.hive.execution.HiveTableScanExec$$anonfun$10.apply(HiveTableScanExec.scala:188)\n\tat org.apache.spark.sql.hive.execution.HiveTableScanExec$$anonfun$10.apply(HiveTableScanExec.scala:188)\n\tat org.apache.spark.util.Utils$.withDummyCallSite(Utils.scala:2516)\n\tat org.apache.spark.sql.hive.execution.HiveTableScanExec.doExecute(HiveTableScanExec.scala:187)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 37 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b79db6b5a067>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_outlet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"outlet_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_outlet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"outlet_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \"\"\"\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o575.count.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange SinglePartition\n+- *(1) HashAggregate(keys=[], functions=[partial_count(1)], output=[count#122L])\n   +- Scan hive mck.ra_outlet_dim_202102 HiveTableRelation `mck`.`ra_outlet_dim_202102`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [outlet_id#65, created_at#66, outlet_name#67, kabupaten#68, longitude#69, latitude#70, outlet_type#71, regional#72, outlet_classification#73, kecamatan#74, kelurahan#75, cluster#76, branch#77, area#78, outlet_category#79, outlet_location_type#80, terakhir_dikunjungi#81, outlet_location_classification#82, rank#83, date#84, month#85, year#86, created_date#87, los#88]\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2830)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2829)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2829)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:745)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2359)\n\tat org.apache.spark.SparkContext.defaultMinPartitions(SparkContext.scala:2368)\n\tat org.apache.spark.sql.hive.HadoopTableReader.<init>(TableReader.scala:82)\n\tat org.apache.spark.sql.hive.execution.HiveTableScanExec.org$apache$spark$sql$hive$execution$HiveTableScanExec$$hadoopReader$lzycompute(HiveTableScanExec.scala:105)\n\tat org.apache.spark.sql.hive.execution.HiveTableScanExec.org$apache$spark$sql$hive$execution$HiveTableScanExec$$hadoopReader(HiveTableScanExec.scala:105)\n\tat org.apache.spark.sql.hive.execution.HiveTableScanExec$$anonfun$10.apply(HiveTableScanExec.scala:188)\n\tat org.apache.spark.sql.hive.execution.HiveTableScanExec$$anonfun$10.apply(HiveTableScanExec.scala:188)\n\tat org.apache.spark.util.Utils$.withDummyCallSite(Utils.scala:2516)\n\tat org.apache.spark.sql.hive.execution.HiveTableScanExec.doExecute(HiveTableScanExec.scala:187)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 37 more\n"
     ]
    }
   ],
   "source": [
    "df_outlet.select(\"outlet_id\").count()\n",
    "df_outlet.select(\"outlet_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_incorrect_lat_long(df):\n",
    "    df = df.filter(\n",
    "        f.col(\"latitude\").cast(\"float\").isNotNull()\n",
    "        & f.col(\"longitude\").cast(\"float\").isNotNull()\n",
    "        & (f.col(\"latitude\") >= -90)\n",
    "        & (f.col(\"latitude\") < 90)\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlet_cleaned = filter_incorrect_lat_long(df_outlet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o633.count.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange SinglePartition\n+- *(1) HashAggregate(keys=[], functions=[partial_count(1)], output=[count#129L])\n   +- *(1) Project\n      +- *(1) Filter (((((isnotnull(latitude#70) && isnotnull(longitude#69)) && isnotnull(cast(latitude#70 as float))) && isnotnull(cast(longitude#69 as float))) && (cast(latitude#70 as int) >= -90)) && (cast(latitude#70 as int) < 90))\n         +- Scan hive mck.ra_outlet_dim_202102 [latitude#70, longitude#69], HiveTableRelation `mck`.`ra_outlet_dim_202102`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [outlet_id#65, created_at#66, outlet_name#67, kabupaten#68, longitude#69, latitude#70, outlet_type#71, regional#72, outlet_classification#73, kecamatan#74, kelurahan#75, cluster#76, branch#77, area#78, outlet_category#79, outlet_location_type#80, terakhir_dikunjungi#81, outlet_location_classification#82, rank#83, date#84, month#85, year#86, created_date#87, los#88]\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2830)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2829)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2829)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:745)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2359)\n\tat org.apache.spark.SparkContext.defaultMinPartitions(SparkContext.scala:2368)\n\tat org.apache.spark.sql.hive.HadoopTableReader.<init>(TableReader.scala:82)\n\tat org.apache.spark.sql.hive.execution.HiveTableScanExec.org$apache$spark$sql$hive$execution$HiveTableScanExec$$hadoopReader$lzycompute(HiveTableScanExec.scala:105)\n\tat org.apache.spark.sql.hive.execution.HiveTableScanExec.org$apache$spark$sql$hive$execution$HiveTableScanExec$$hadoopReader(HiveTableScanExec.scala:105)\n\tat org.apache.spark.sql.hive.execution.HiveTableScanExec$$anonfun$10.apply(HiveTableScanExec.scala:188)\n\tat org.apache.spark.sql.hive.execution.HiveTableScanExec$$anonfun$10.apply(HiveTableScanExec.scala:188)\n\tat org.apache.spark.util.Utils$.withDummyCallSite(Utils.scala:2516)\n\tat org.apache.spark.sql.hive.execution.HiveTableScanExec.doExecute(HiveTableScanExec.scala:187)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:121)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 37 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-16186debbe0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_outlet_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"outlet_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_outlet_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"outlet_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \"\"\"\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o633.count.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange SinglePartition\n+- *(1) HashAggregate(keys=[], functions=[partial_count(1)], output=[count#129L])\n   +- *(1) Project\n      +- *(1) Filter (((((isnotnull(latitude#70) && isnotnull(longitude#69)) && isnotnull(cast(latitude#70 as float))) && isnotnull(cast(longitude#69 as float))) && (cast(latitude#70 as int) >= -90)) && (cast(latitude#70 as int) < 90))\n         +- Scan hive mck.ra_outlet_dim_202102 [latitude#70, longitude#69], HiveTableRelation `mck`.`ra_outlet_dim_202102`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [outlet_id#65, created_at#66, outlet_name#67, kabupaten#68, longitude#69, latitude#70, outlet_type#71, regional#72, outlet_classification#73, kecamatan#74, kelurahan#75, cluster#76, branch#77, area#78, outlet_category#79, outlet_location_type#80, terakhir_dikunjungi#81, outlet_location_classification#82, rank#83, date#84, month#85, year#86, created_date#87, los#88]\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2830)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2829)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2829)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:745)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2359)\n\tat org.apache.spark.SparkContext.defaultMinPartitions(SparkContext.scala:2368)\n\tat org.apache.spark.sql.hive.HadoopTableReader.<init>(TableReader.scala:82)\n\tat org.apache.spark.sql.hive.execution.HiveTableScanExec.org$apache$spark$sql$hive$execution$HiveTableScanExec$$hadoopReader$lzycompute(HiveTableScanExec.scala:105)\n\tat org.apache.spark.sql.hive.execution.HiveTableScanExec.org$apache$spark$sql$hive$execution$HiveTableScanExec$$hadoopReader(HiveTableScanExec.scala:105)\n\tat org.apache.spark.sql.hive.execution.HiveTableScanExec$$anonfun$10.apply(HiveTableScanExec.scala:188)\n\tat org.apache.spark.sql.hive.execution.HiveTableScanExec$$anonfun$10.apply(HiveTableScanExec.scala:188)\n\tat org.apache.spark.util.Utils$.withDummyCallSite(Utils.scala:2516)\n\tat org.apache.spark.sql.hive.execution.HiveTableScanExec.doExecute(HiveTableScanExec.scala:187)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:121)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 37 more\n"
     ]
    }
   ],
   "source": [
    "df_outlet_cleaned.select(\"outlet_id\").count()\n",
    "df_outlet_cleaned.select(\"outlet_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_all_neighbours_for_outlet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3789e80886cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_outlet_neighbours\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_neighbours_for_outlet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_outlet_cleaned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_all_neighbours_for_outlet' is not defined"
     ]
    }
   ],
   "source": [
    "df_outlet_neighbours = get_all_neighbours_for_outlet(df_outlet_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[outlet_id: string, lat1: float, long1: float, neighbours_all: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_outlet_neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poi = spark.read.table(\"mck.poi_places_here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- placeid: string (nullable = true)\n",
      " |-- countrycode: string (nullable = true)\n",
      " |-- provinsi: string (nullable = true)\n",
      " |-- kabupaten: string (nullable = true)\n",
      " |-- kecamatan: string (nullable = true)\n",
      " |-- postalcode: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- namepoi: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- streetname: string (nullable = true)\n",
      " |-- qualitylevel: string (nullable = true)\n",
      " |-- level_1_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_poi.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8606776"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_poi.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poi = df_poi.filter(f.col(\"level_1_name\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6536487"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_poi_cleaned = filter_incorrect_lat_long(df_poi)\n",
    "df_poi_cleaned.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poi_cleaned = df_poi_cleaned.select(\"latitude\",\"longitude\",\"namepoi\",f.trim(f.lower(f.col((\"level_1_name\")))).alias(\"level_1_name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poi_neighbours = get_all_neighbours_for_outlet(df_poi_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poi_neighbours =  df_poi_neighbours.withColumnRenamed(\"lat1\",\"lat2\").withColumnRenamed(\"long1\",\"long2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58828383"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_poi_neighbours.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_outlet_poi = df_outlet_neighbours.join(\n",
    "                df_poi_neighbours, [\"neighbours_all\"], how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_columns = [\"outlet_id\", \"lat1\", \"long1\", \"lat2\", \"long2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group_join_outlet_poi = df_join_outlet_poi.groupBy(group_by_columns).agg(f.max(\"namepoi\").alias(\"namepoi\"),f.max(\"level_1_name\").alias(\"level_1_name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Distance & Calculate Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance_bw_2_lat_long(df):\n",
    "    df = (\n",
    "        df.withColumn(\n",
    "            \"a\",\n",
    "            (\n",
    "                f.pow(f.sin(f.radians(f.col(\"lat2\") - f.col(\"lat1\")) / 2), 2)\n",
    "                + f.cos(f.radians(f.col(\"lat1\")))\n",
    "                * f.cos(f.radians(f.col(\"lat2\")))\n",
    "                * f.pow(f.sin(f.radians(f.col(\"long2\") - f.col(\"long1\")) / 2), 2)\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"distance\", f.round(f.atan2(f.sqrt(f.col(\"a\")), f.sqrt(-f.col(\"a\") + 1)) * 12742000,2)\n",
    "        )\n",
    "        .drop(\"a\")\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group_join_outlet_poi_with_distance = calc_distance_bw_2_lat_long(df_group_join_outlet_poi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "radiuses_in_m = [100, 250, 500, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for radius in radiuses_in_m:\n",
    "    df_group_join_outlet_poi_with_distance = df_group_join_outlet_poi_with_distance.withColumn(\n",
    "        f\"under_{radius}m\", (f.col(\"distance\") <= radius).cast(\"integer\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[outlet_id: string, lat1: float, long1: float, lat2: float, long2: float, namepoi: string, level_1_name: string, distance: double, under_100m: int, under_250m: int, under_500m: int, under_1000m: int]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group_join_outlet_poi_with_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fea_radius = (df_group_join_outlet_poi_with_distance.groupBy(\"outlet_id\").agg(\n",
    "    #leisure and outdoor\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='leisure and outdoor',f.col(\"under_100m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_100m_to_leisure_and_outdoor\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='leisure and outdoor',f.col(\"under_250m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_250m_to_leisure_and_outdoor\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='leisure and outdoor',f.col(\"under_500m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_500m_to_leisure_and_outdoor\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='leisure and outdoor',f.col(\"under_1000m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_1000m_to_leisure_and_outdoor\"),\n",
    "    #areas and buildings\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='areas and buildings',f.col(\"under_100m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_100m_to_areas_and_buildings\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='areas and buildings',f.col(\"under_250m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_250m_to_areas_and_buildings\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='areas and buildings',f.col(\"under_500m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_500m_to_areas_and_buildings\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='areas and buildings',f.col(\"under_1000m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_1000m_to_areas_and_buildings\"),\n",
    "    #shopping \n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='shopping',f.col(\"under_100m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_100m_to_shopping\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='shopping',f.col(\"under_250m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_250m_to_shopping\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='shopping',f.col(\"under_500m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_500m_to_shopping\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='shopping',f.col(\"under_1000m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_1000m_to_shopping\"),\n",
    "    #going out-entertainment\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='going out-entertainment',f.col(\"under_100m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_100m_to_going_out_entertainment\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='going out-entertainment',f.col(\"under_250m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_250m_to_going_out_entertainment\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='going out-entertainment',f.col(\"under_500m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_500m_to_going_out_entertainment\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='going out-entertainment',f.col(\"under_1000m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_1000m_to_going_out_entertainment\"),\n",
    "    #sights and museums\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='sights and museums',f.col(\"under_100m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_100m_to_sights_and_museums\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='sights and museums',f.col(\"under_250m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_250m_to_sights_and_museums\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='sights and museums',f.col(\"under_500m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_500m_to_sights_and_museums\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='sights and museums',f.col(\"under_1000m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_1000m_to_sights_and_museums\"),\n",
    "    #facilities \n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='facilities',f.col(\"under_100m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_100m_to_facilities\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='facilities',f.col(\"under_250m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_250m_to_facilities\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='facilities',f.col(\"under_500m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_500m_to_facilities\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='facilities',f.col(\"under_1000m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_1000m_to_facilities\"),\n",
    "    #accommodation \n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='accommodation',f.col(\"under_100m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_100m_to_accommodation\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='accommodation',f.col(\"under_250m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_250m_to_accommodation\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='accommodation',f.col(\"under_500m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_500m_to_accommodation\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='accommodation',f.col(\"under_1000m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_1000m_to_accommodation\"),\n",
    "    #accommodation \n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='natural and geographical',f.col(\"under_100m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_100m_to_natural_and_geographical\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='natural and geographical',f.col(\"under_250m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_250m_to_natural_and_geographical\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='natural and geographical',f.col(\"under_500m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_500m_to_natural_and_geographical\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='natural and geographical',f.col(\"under_1000m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_1000m_to_natural_and_geographical\"),\n",
    "    #business and services \n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='business and services',f.col(\"under_100m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_100m_to_business_and_services\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='business and services',f.col(\"under_250m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_250m_to_business_and_services\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='business and services',f.col(\"under_500m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_500m_to_business_and_services\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='business and services',f.col(\"under_1000m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_1000m_to_business_and_services\"),\n",
    "    #transport\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='transport',f.col(\"under_100m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_100m_to_transport\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='transport',f.col(\"under_250m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_250m_to_transport\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='transport',f.col(\"under_500m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_500m_to_transport\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='transport',f.col(\"under_1000m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_1000m_to_transport\"),\n",
    "    #transport\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='eat and drink',f.col(\"under_100m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_100m_to_eat_and_drink\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='eat and drink',f.col(\"under_250m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_250m_to_eat_and_drink\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='eat and drink',f.col(\"under_500m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_500m_to_eat_and_drink\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='eat and drink',f.col(\"under_1000m\")).otherwise(f.lit(0))).alias(\"fea_outlet_under_1000m_to_eat_and_drink\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shortest Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lower, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_outlet_poi_with_distance = Window.partitionBy(\"outlet_id\",\"level_1_name\").orderBy(f.col(\"distance\"))\n",
    "df_nearest_outlet_poi_with_distance = df_group_join_outlet_poi_with_distance.withColumn(\n",
    "        \"row_number\", f.row_number().over(window_outlet_poi_with_distance)\n",
    ").filter(f.col(\"row_number\") == 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fea_nearest_distance = (df_nearest_outlet_poi_with_distance.groupBy(\"outlet_id\").agg(\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='leisure and outdoor',f.col(\"distance\")).otherwise(f.lit(0))).alias(\"fea_min_distance_outlet_to_leisure_and_outdoor\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='areas and buildings',f.col(\"distance\")).otherwise(f.lit(0))).alias(\"fea_min_distance_outlet_to_areas_and_buildings\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='shopping',f.col(\"distance\")).otherwise(f.lit(0))).alias(\"fea_min_distance_outlet_to_shopping\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='going out-entertainment',f.col(\"distance\")).otherwise(f.lit(0))).alias(\"fea_min_distance_outlet_to_going_out_entertainment\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='sights and museums',f.col(\"distance\")).otherwise(f.lit(0))).alias(\"fea_min_distance_outlet_to_sights_and_museums\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='facilities',f.col(\"distance\")).otherwise(f.lit(0))).alias(\"fea_min_distance_outlet_to_facilities\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='accommodation',f.col(\"distance\")).otherwise(f.lit(0))).alias(\"fea_min_distance_outlet_to_accommodation\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='natural and geographical',f.col(\"distance\")).otherwise(f.lit(0))).alias(\"fea_min_distance_outlet_to_natural_and_geographical\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='business and services',f.col(\"distance\")).otherwise(f.lit(0))).alias(\"fea_min_distance_outlet_to_business_and_services\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='transport',f.col(\"distance\")).otherwise(f.lit(0))).alias(\"fea_min_distance_outlet_to_transport\"),\n",
    "    f.sum(f.when(f.col(\"level_1_name\")=='eat and drink',f.col(\"distance\")).otherwise(f.lit(0))).alias(\"fea_min_distance_outlet_to_eat_and_drink\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[outlet_id: string, fea_min_distance_outlet_to_leisure_and_outdoor: double, fea_min_distance_outlet_to_areas_and_buildings: double, fea_min_distance_outlet_to_shopping: double, fea_min_distance_outlet_to_going_out_entertainment: double, fea_min_distance_outlet_to_sights_and_museums: double, fea_min_distance_outlet_to_facilities: double, fea_min_distance_outlet_to_accommodation: double, fea_min_distance_outlet_to_natural_and_geographical: double, fea_min_distance_outlet_to_business_and_services: double, fea_min_distance_outlet_to_transport: double, fea_min_distance_outlet_to_eat_and_drink: double]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fea_nearest_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join & Write Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlet_cleaned_join = df_outlet_cleaned.select(\"outlet_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlet_cleaned_join = df_outlet_cleaned_join.join(df_fea_nearest_distance,[\"outlet_id\"],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlet_cleaned_join = df_outlet_cleaned_join.join(df_fea_radius,[\"outlet_id\"],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_outlet_cleaned_join.coalesce(1).write.mode(\"overwrite\").parquet(\"/data/landing/gx_pnt/users/natan/here/fea_outlet_radius_and_minimum_distance_places_ambarawa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ambarawa = spark.read.parquet(\"/data/landing/gx_pnt/users/natan/here/fea_outlet_radius_and_minimum_distance_places_ambarawa\")\n",
    "# ambarawa.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlet_cleaned_join.repartition(100).write.mode(\"overwrite\").parquet(\"/data/landing/gx_pnt/users/natan/here/fea_outlet_radius_and_minimum_distance_places\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "489085"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "489085"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_outlet_radius_and_minimum_distance_places = spark.read.parquet(\"/data/landing/gx_pnt/users/natan/here/fea_outlet_radius_and_minimum_distance_places\")\n",
    "fea_outlet_radius_and_minimum_distance_places.select(\"outlet_id\").distinct().count()\n",
    "fea_outlet_radius_and_minimum_distance_places.select(\"outlet_id\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geohash Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from typing import Any, Dict, Iterable, List\n",
    "from pyspark.sql.types import ArrayType, StringType, StructField, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_udf = f.udf(lambda x, y, z: encode(float(x), float(y), z))\n",
    "neighbors_udf = f.udf(lambda geohash: neighbors(geohash), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbours(df: pyspark.sql.DataFrame):\n",
    "    df = filter_incorrect_lat_long(df)\n",
    "    columns_of_interest = df.columns + [\"neighbours_all\"]\n",
    "    df_with_neighbors = (\n",
    "        df.withColumn(\n",
    "            \"geohash\", geo_udf(f.col(\"latitude\"), f.col(\"longitude\"), f.lit(5))\n",
    "        )\n",
    "        .withColumn(\"neighbours\", neighbors_udf(f.col(\"geohash\")))\n",
    "        .withColumn(\n",
    "            \"neighbours_all\", f.array_union(f.array(\"geohash\"), f.col(\"neighbours\"))\n",
    "        )\n",
    "        .select(columns_of_interest)\n",
    "    )\n",
    "    return df_with_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_neighbours_for_outlet(\n",
    "    df_outlet: pyspark.sql.DataFrame,\n",
    ") -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    Gets all neighbouring geohashes for an outlet geohash.\n",
    "\n",
    "    :param df_outlet: Outlet raw data.\n",
    "    :return: 9 rows per outlet as there are 9 neigbouring geohashes for each\n",
    "    outlet including the geohash in which the outlet is.\n",
    "    \"\"\"\n",
    "    df_outlet_with_neighbors = get_neighbours(df_outlet)\n",
    "    df_outlet_neighbours_exploded = df_outlet_with_neighbors.withColumn(\n",
    "        \"neighbours_all\", f.explode(\"neighbours_all\")\n",
    "    )\n",
    "    df_outlet_neighbours_exploded = df_outlet_neighbours_exploded.withColumn(\n",
    "        \"latitude\", f.col(\"latitude\").cast(\"float\")\n",
    "    ).withColumn(\"longitude\", f.col(\"longitude\").cast(\"float\"))\n",
    "\n",
    "    old_new_column_names_dict = {\n",
    "        \"latitude\": \"lat1\",\n",
    "        \"longitude\": \"long1\",\n",
    "    }\n",
    "    df_outlet_neighbours_exploded = rename_columns(\n",
    "        df_outlet_neighbours_exploded, old_new_column_names_dict\n",
    "    )\n",
    "    return df_outlet_neighbours_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df: pyspark.sql.DataFrame, old_new_column_names_dict: Dict):\n",
    "    for old_col_name, new_col_name in old_new_column_names_dict.items():\n",
    "        df = df.withColumnRenamed(old_col_name, new_col_name)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2018-present QuantumBlack Visual Analytics Limited\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n",
    "# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\n",
    "# OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND\n",
    "# NONINFRINGEMENT. IN NO EVENT WILL THE LICENSOR OR OTHER CONTRIBUTORS\n",
    "# BE LIABLE FOR ANY CLAIM, DAMAGES, OR OTHER LIABILITY, WHETHER IN AN\n",
    "# ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF, OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "#\n",
    "# The QuantumBlack Visual Analytics Limited (\"QuantumBlack\") name and logo\n",
    "# (either separately or in combination, \"QuantumBlack Trademarks\") are\n",
    "# trademarks of QuantumBlack. The License does not grant you any right or\n",
    "# license to the QuantumBlack Trademarks. You may not use the QuantumBlack\n",
    "# Trademarks or any confusingly similar mark as a trademark for your product,\n",
    "#     or use the QuantumBlack Trademarks in any other manner that might cause\n",
    "# confusion in the marketplace, including but not limited to in advertising,\n",
    "# on websites, or on software.\n",
    "#\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# coding: UTF-8\n",
    "\n",
    "try:\n",
    "    import _geohash\n",
    "except ImportError:\n",
    "    _geohash = None\n",
    "\n",
    "__version__ = \"0.8.5\"\n",
    "__all__ = [\"encode\", \"decode\", \"decode_exactly\", \"bbox\", \"neighbors\", \"expand\"]\n",
    "\n",
    "_base32 = \"0123456789bcdefghjkmnpqrstuvwxyz\"\n",
    "_base32_map = {}\n",
    "for i in range(len(_base32)):\n",
    "    _base32_map[_base32[i]] = i\n",
    "del i\n",
    "\n",
    "LONG_ZERO = 0\n",
    "import sys\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    LONG_ZERO = int(0)\n",
    "\n",
    "\n",
    "def _float_hex_to_int(f):\n",
    "    if f < -1.0 or f >= 1.0:\n",
    "        return None\n",
    "\n",
    "    if f == 0.0:\n",
    "        return 1, 1\n",
    "\n",
    "    h = f.hex()\n",
    "    x = h.find(\"0x1.\")\n",
    "    assert x >= 0\n",
    "    p = h.find(\"p\")\n",
    "    assert p > 0\n",
    "\n",
    "    half_len = len(h[x + 4 : p]) * 4 - int(h[p + 1 :])\n",
    "    if x == 0:\n",
    "        r = (1 << half_len) + ((1 << (len(h[x + 4 : p]) * 4)) + int(h[x + 4 : p], 16))\n",
    "    else:\n",
    "        r = (1 << half_len) - ((1 << (len(h[x + 4 : p]) * 4)) + int(h[x + 4 : p], 16))\n",
    "\n",
    "    return r, half_len + 1\n",
    "\n",
    "\n",
    "def _int_to_float_hex(i, l):\n",
    "    if l == 0:\n",
    "        return -1.0\n",
    "\n",
    "    half = 1 << (l - 1)\n",
    "    s = int((l + 3) / 4)\n",
    "    if i >= half:\n",
    "        i = i - half\n",
    "        return float.fromhex((\"0x0.%0\" + str(s) + \"xp1\") % (i << (s * 4 - l),))\n",
    "    else:\n",
    "        i = half - i\n",
    "        return float.fromhex((\"-0x0.%0\" + str(s) + \"xp1\") % (i << (s * 4 - l),))\n",
    "\n",
    "\n",
    "def _encode_i2c(lat, lon, lat_length, lon_length):\n",
    "    precision = int((lat_length + lon_length) / 5)\n",
    "    if lat_length < lon_length:\n",
    "        a = lon\n",
    "        b = lat\n",
    "    else:\n",
    "        a = lat\n",
    "        b = lon\n",
    "\n",
    "    boost = (0, 1, 4, 5, 16, 17, 20, 21)\n",
    "    ret = \"\"\n",
    "    for i in range(precision):\n",
    "        ret += _base32[(boost[a & 7] + (boost[b & 3] << 1)) & 0x1F]\n",
    "        t = a >> 3\n",
    "        a = b >> 2\n",
    "        b = t\n",
    "\n",
    "    return ret[::-1]\n",
    "\n",
    "\n",
    "def encode(latitude, longitude, precision=12):\n",
    "    if latitude >= 90.0 or latitude < -90.0:\n",
    "        raise Exception(\"invalid latitude.\")\n",
    "    while longitude < -180.0:\n",
    "        longitude += 360.0\n",
    "    while longitude >= 180.0:\n",
    "        longitude -= 360.0\n",
    "\n",
    "    if _geohash:\n",
    "        basecode = _geohash.encode(latitude, longitude)\n",
    "        if len(basecode) > precision:\n",
    "            return basecode[0:precision]\n",
    "        return basecode + \"0\" * (precision - len(basecode))\n",
    "\n",
    "    xprecision = precision + 1\n",
    "    lat_length = lon_length = int(xprecision * 5 / 2)\n",
    "    if xprecision % 2 == 1:\n",
    "        lon_length += 1\n",
    "\n",
    "    if hasattr(float, \"fromhex\"):\n",
    "        a = _float_hex_to_int(latitude / 90.0)\n",
    "        o = _float_hex_to_int(longitude / 180.0)\n",
    "        if a[1] > lat_length:\n",
    "            ai = a[0] >> (a[1] - lat_length)\n",
    "        else:\n",
    "            ai = a[0] << (lat_length - a[1])\n",
    "\n",
    "        if o[1] > lon_length:\n",
    "            oi = o[0] >> (o[1] - lon_length)\n",
    "        else:\n",
    "            oi = o[0] << (lon_length - o[1])\n",
    "\n",
    "        return _encode_i2c(ai, oi, lat_length, lon_length)[:precision]\n",
    "\n",
    "    lat = latitude / 180.0\n",
    "    lon = longitude / 360.0\n",
    "\n",
    "    if lat > 0:\n",
    "        lat = int((1 << lat_length) * lat) + (1 << (lat_length - 1))\n",
    "    else:\n",
    "        lat = (1 << lat_length - 1) - int((1 << lat_length) * (-lat))\n",
    "\n",
    "    if lon > 0:\n",
    "        lon = int((1 << lon_length) * lon) + (1 << (lon_length - 1))\n",
    "    else:\n",
    "        lon = (1 << lon_length - 1) - int((1 << lon_length) * (-lon))\n",
    "\n",
    "    return _encode_i2c(lat, lon, lat_length, lon_length)[:precision]\n",
    "\n",
    "\n",
    "def _decode_c2i(hashcode):\n",
    "    lon = 0\n",
    "    lat = 0\n",
    "    bit_length = 0\n",
    "    lat_length = 0\n",
    "    lon_length = 0\n",
    "    for i in hashcode:\n",
    "        t = _base32_map[i]\n",
    "        if bit_length % 2 == 0:\n",
    "            lon = lon << 3\n",
    "            lat = lat << 2\n",
    "            lon += (t >> 2) & 4\n",
    "            lat += (t >> 2) & 2\n",
    "            lon += (t >> 1) & 2\n",
    "            lat += (t >> 1) & 1\n",
    "            lon += t & 1\n",
    "            lon_length += 3\n",
    "            lat_length += 2\n",
    "        else:\n",
    "            lon = lon << 2\n",
    "            lat = lat << 3\n",
    "            lat += (t >> 2) & 4\n",
    "            lon += (t >> 2) & 2\n",
    "            lat += (t >> 1) & 2\n",
    "            lon += (t >> 1) & 1\n",
    "            lat += t & 1\n",
    "            lon_length += 2\n",
    "            lat_length += 3\n",
    "\n",
    "        bit_length += 5\n",
    "\n",
    "    return (lat, lon, lat_length, lon_length)\n",
    "\n",
    "\n",
    "def decode(hashcode, delta=False):\n",
    "    \"\"\"\n",
    "    decode a hashcode and get center coordinate, and distance between center and outer border\n",
    "    \"\"\"\n",
    "    if _geohash:\n",
    "        (lat, lon, lat_bits, lon_bits) = _geohash.decode(hashcode)\n",
    "        latitude_delta = 90.0 / (1 << lat_bits)\n",
    "        longitude_delta = 180.0 / (1 << lon_bits)\n",
    "        latitude = lat + latitude_delta\n",
    "        longitude = lon + longitude_delta\n",
    "        if delta:\n",
    "            return latitude, longitude, latitude_delta, longitude_delta\n",
    "        return latitude, longitude\n",
    "\n",
    "    (lat, lon, lat_length, lon_length) = _decode_c2i(hashcode)\n",
    "\n",
    "    if hasattr(float, \"fromhex\"):\n",
    "        latitude_delta = 90.0 / (1 << lat_length)\n",
    "        longitude_delta = 180.0 / (1 << lon_length)\n",
    "        latitude = _int_to_float_hex(lat, lat_length) * 90.0 + latitude_delta\n",
    "        longitude = _int_to_float_hex(lon, lon_length) * 180.0 + longitude_delta\n",
    "        if delta:\n",
    "            return latitude, longitude, latitude_delta, longitude_delta\n",
    "        return latitude, longitude\n",
    "\n",
    "    lat = (lat << 1) + 1\n",
    "    lon = (lon << 1) + 1\n",
    "    lat_length += 1\n",
    "    lon_length += 1\n",
    "\n",
    "    latitude = 180.0 * (lat - (1 << (lat_length - 1))) / (1 << lat_length)\n",
    "    longitude = 360.0 * (lon - (1 << (lon_length - 1))) / (1 << lon_length)\n",
    "    if delta:\n",
    "        latitude_delta = 180.0 / (1 << lat_length)\n",
    "        longitude_delta = 360.0 / (1 << lon_length)\n",
    "        return latitude, longitude, latitude_delta, longitude_delta\n",
    "\n",
    "    return latitude, longitude\n",
    "\n",
    "\n",
    "def decode_exactly(hashcode):\n",
    "    return decode(hashcode, True)\n",
    "\n",
    "\n",
    "## hashcode operations below\n",
    "\n",
    "\n",
    "def bbox(hashcode):\n",
    "    \"\"\"\n",
    "    decode a hashcode and get north, south, east and west border.\n",
    "    \"\"\"\n",
    "    if _geohash:\n",
    "        (lat, lon, lat_bits, lon_bits) = _geohash.decode(hashcode)\n",
    "        latitude_delta = 180.0 / (1 << lat_bits)\n",
    "        longitude_delta = 360.0 / (1 << lon_bits)\n",
    "        return {\n",
    "            \"s\": lat,\n",
    "            \"w\": lon,\n",
    "            \"n\": lat + latitude_delta,\n",
    "            \"e\": lon + longitude_delta,\n",
    "        }\n",
    "\n",
    "    (lat, lon, lat_length, lon_length) = _decode_c2i(hashcode)\n",
    "    if hasattr(float, \"fromhex\"):\n",
    "        latitude_delta = 180.0 / (1 << lat_length)\n",
    "        longitude_delta = 360.0 / (1 << lon_length)\n",
    "        latitude = _int_to_float_hex(lat, lat_length) * 90.0\n",
    "        longitude = _int_to_float_hex(lon, lon_length) * 180.0\n",
    "        return {\n",
    "            \"s\": latitude,\n",
    "            \"w\": longitude,\n",
    "            \"n\": latitude + latitude_delta,\n",
    "            \"e\": longitude + longitude_delta,\n",
    "        }\n",
    "\n",
    "    ret = {}\n",
    "    if lat_length:\n",
    "        ret[\"n\"] = 180.0 * (lat + 1 - (1 << (lat_length - 1))) / (1 << lat_length)\n",
    "        ret[\"s\"] = 180.0 * (lat - (1 << (lat_length - 1))) / (1 << lat_length)\n",
    "    else:  # can't calculate the half with bit shifts (negative shift)\n",
    "        ret[\"n\"] = 90.0\n",
    "        ret[\"s\"] = -90.0\n",
    "\n",
    "    if lon_length:\n",
    "        ret[\"e\"] = 360.0 * (lon + 1 - (1 << (lon_length - 1))) / (1 << lon_length)\n",
    "        ret[\"w\"] = 360.0 * (lon - (1 << (lon_length - 1))) / (1 << lon_length)\n",
    "    else:  # can't calculate the half with bit shifts (negative shift)\n",
    "        ret[\"e\"] = 180.0\n",
    "        ret[\"w\"] = -180.0\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def neighbors(hashcode):\n",
    "    if _geohash and len(hashcode) < 25:\n",
    "        return _geohash.neighbors(hashcode)\n",
    "\n",
    "    (lat, lon, lat_length, lon_length) = _decode_c2i(hashcode)\n",
    "    ret = []\n",
    "    tlat = lat\n",
    "    for tlon in (lon - 1, lon + 1):\n",
    "        code = _encode_i2c(tlat, tlon, lat_length, lon_length)\n",
    "        if code:\n",
    "            ret.append(code)\n",
    "\n",
    "    tlat = lat + 1\n",
    "    if not tlat >> lat_length:\n",
    "        for tlon in (lon - 1, lon, lon + 1):\n",
    "            ret.append(_encode_i2c(tlat, tlon, lat_length, lon_length))\n",
    "\n",
    "    tlat = lat - 1\n",
    "    if tlat >= 0:\n",
    "        for tlon in (lon - 1, lon, lon + 1):\n",
    "            ret.append(_encode_i2c(tlat, tlon, lat_length, lon_length))\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def expand(hashcode):\n",
    "    ret = neighbors(hashcode)\n",
    "    ret.append(hashcode)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def _uint64_interleave(lat32, lon32):\n",
    "    intr = 0\n",
    "    boost = (0, 1, 4, 5, 16, 17, 20, 21, 64, 65, 68, 69, 80, 81, 84, 85)\n",
    "    for i in range(8):\n",
    "        intr = (\n",
    "            (intr << 8)\n",
    "            + (boost[(lon32 >> (28 - i * 4)) % 16] << 1)\n",
    "            + boost[(lat32 >> (28 - i * 4)) % 16]\n",
    "        )\n",
    "\n",
    "    return intr\n",
    "\n",
    "\n",
    "def _uint64_deinterleave(ui64):\n",
    "    lat = lon = 0\n",
    "    boost = (\n",
    "        (0, 0),\n",
    "        (0, 1),\n",
    "        (1, 0),\n",
    "        (1, 1),\n",
    "        (0, 2),\n",
    "        (0, 3),\n",
    "        (1, 2),\n",
    "        (1, 3),\n",
    "        (2, 0),\n",
    "        (2, 1),\n",
    "        (3, 0),\n",
    "        (3, 1),\n",
    "        (2, 2),\n",
    "        (2, 3),\n",
    "        (3, 2),\n",
    "        (3, 3),\n",
    "    )\n",
    "    for i in range(16):\n",
    "        p = boost[(ui64 >> (60 - i * 4)) % 16]\n",
    "        lon = (lon << 2) + p[0]\n",
    "        lat = (lat << 2) + p[1]\n",
    "\n",
    "    return (lat, lon)\n",
    "\n",
    "\n",
    "def encode_uint64(latitude, longitude):\n",
    "    if latitude >= 90.0 or latitude < -90.0:\n",
    "        raise ValueError(\"Latitude must be in the range of (-90.0, 90.0)\")\n",
    "    while longitude < -180.0:\n",
    "        longitude += 360.0\n",
    "    while longitude >= 180.0:\n",
    "        longitude -= 360.0\n",
    "\n",
    "    if _geohash:\n",
    "        ui128 = _geohash.encode_int(latitude, longitude)\n",
    "        if _geohash.intunit == 64:\n",
    "            return ui128[0]\n",
    "        elif _geohash.intunit == 32:\n",
    "            return (ui128[0] << 32) + ui128[1]\n",
    "        elif _geohash.intunit == 16:\n",
    "            return (ui128[0] << 48) + (ui128[1] << 32) + (ui128[2] << 16) + ui128[3]\n",
    "\n",
    "    lat = int(((latitude + 90.0) / 180.0) * (1 << 32))\n",
    "    lon = int(((longitude + 180.0) / 360.0) * (1 << 32))\n",
    "    return _uint64_interleave(lat, lon)\n",
    "\n",
    "\n",
    "def decode_uint64(ui64):\n",
    "    if _geohash:\n",
    "        latlon = _geohash.decode_int(ui64 % 0xFFFFFFFFFFFFFFFF, LONG_ZERO)\n",
    "        if latlon:\n",
    "            return latlon\n",
    "\n",
    "    lat, lon = _uint64_deinterleave(ui64)\n",
    "    return (180.0 * lat / (1 << 32) - 90.0, 360.0 * lon / (1 << 32) - 180.0)\n",
    "\n",
    "\n",
    "def expand_uint64(ui64, precision=50):\n",
    "    ui64 = ui64 & (0xFFFFFFFFFFFFFFFF << (64 - precision))\n",
    "    lat, lon = _uint64_deinterleave(ui64)\n",
    "    lat_grid = 1 << (32 - int(precision / 2))\n",
    "    lon_grid = lat_grid >> (precision % 2)\n",
    "\n",
    "    if precision <= 2:  # expand becomes to the whole range\n",
    "        return []\n",
    "\n",
    "    ranges = []\n",
    "    if lat & lat_grid:\n",
    "        if lon & lon_grid:\n",
    "            ui64 = _uint64_interleave(lat - lat_grid, lon - lon_grid)\n",
    "            ranges.append((ui64, ui64 + (1 << (64 - precision + 2))))\n",
    "            if precision % 2 == 0:\n",
    "                # lat,lon = (1, 1) and even precision\n",
    "                ui64 = _uint64_interleave(lat - lat_grid, lon + lon_grid)\n",
    "                ranges.append((ui64, ui64 + (1 << (64 - precision + 1))))\n",
    "\n",
    "                if lat + lat_grid < 0xFFFFFFFF:\n",
    "                    ui64 = _uint64_interleave(lat + lat_grid, lon - lon_grid)\n",
    "                    ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "                    ui64 = _uint64_interleave(lat + lat_grid, lon)\n",
    "                    ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "                    ui64 = _uint64_interleave(lat + lat_grid, lon + lon_grid)\n",
    "                    ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "            else:\n",
    "                # lat,lon = (1, 1) and odd precision\n",
    "                if lat + lat_grid < 0xFFFFFFFF:\n",
    "                    ui64 = _uint64_interleave(lat + lat_grid, lon - lon_grid)\n",
    "                    ranges.append((ui64, ui64 + (1 << (64 - precision + 1))))\n",
    "\n",
    "                    ui64 = _uint64_interleave(lat + lat_grid, lon + lon_grid)\n",
    "                    ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "\n",
    "                ui64 = _uint64_interleave(lat, lon + lon_grid)\n",
    "                ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "                ui64 = _uint64_interleave(lat - lat_grid, lon + lon_grid)\n",
    "                ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "        else:\n",
    "            ui64 = _uint64_interleave(lat - lat_grid, lon)\n",
    "            ranges.append((ui64, ui64 + (1 << (64 - precision + 2))))\n",
    "            if precision % 2 == 0:\n",
    "                # lat,lon = (1, 0) and odd precision\n",
    "                ui64 = _uint64_interleave(lat - lat_grid, lon - lon_grid)\n",
    "                ranges.append((ui64, ui64 + (1 << (64 - precision + 1))))\n",
    "\n",
    "                if lat + lat_grid < 0xFFFFFFFF:\n",
    "                    ui64 = _uint64_interleave(lat + lat_grid, lon - lon_grid)\n",
    "                    ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "                    ui64 = _uint64_interleave(lat + lat_grid, lon)\n",
    "                    ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "                    ui64 = _uint64_interleave(lat + lat_grid, lon + lon_grid)\n",
    "                    ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "            else:\n",
    "                # lat,lon = (1, 0) and odd precision\n",
    "                if lat + lat_grid < 0xFFFFFFFF:\n",
    "                    ui64 = _uint64_interleave(lat + lat_grid, lon)\n",
    "                    ranges.append((ui64, ui64 + (1 << (64 - precision + 1))))\n",
    "\n",
    "                    ui64 = _uint64_interleave(lat + lat_grid, lon - lon_grid)\n",
    "                    ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "                ui64 = _uint64_interleave(lat, lon - lon_grid)\n",
    "                ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "                ui64 = _uint64_interleave(lat - lat_grid, lon - lon_grid)\n",
    "                ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "    else:\n",
    "        if lon & lon_grid:\n",
    "            ui64 = _uint64_interleave(lat, lon - lon_grid)\n",
    "            ranges.append((ui64, ui64 + (1 << (64 - precision + 2))))\n",
    "            if precision % 2 == 0:\n",
    "                # lat,lon = (0, 1) and even precision\n",
    "                ui64 = _uint64_interleave(lat, lon + lon_grid)\n",
    "                ranges.append((ui64, ui64 + (1 << (64 - precision + 1))))\n",
    "\n",
    "                if lat > 0:\n",
    "                    ui64 = _uint64_interleave(lat - lat_grid, lon - lon_grid)\n",
    "                    ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "                    ui64 = _uint64_interleave(lat - lat_grid, lon)\n",
    "                    ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "                    ui64 = _uint64_interleave(lat - lat_grid, lon + lon_grid)\n",
    "                    ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "            else:\n",
    "                # lat,lon = (0, 1) and odd precision\n",
    "                if lat > 0:\n",
    "                    ui64 = _uint64_interleave(lat - lat_grid, lon - lon_grid)\n",
    "                    ranges.append((ui64, ui64 + (1 << (64 - precision + 1))))\n",
    "\n",
    "                    ui64 = _uint64_interleave(lat - lat_grid, lon + lon_grid)\n",
    "                    ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "                ui64 = _uint64_interleave(lat, lon + lon_grid)\n",
    "                ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "                ui64 = _uint64_interleave(lat + lat_grid, lon + lon_grid)\n",
    "                ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "        else:\n",
    "            ui64 = _uint64_interleave(lat, lon)\n",
    "            ranges.append((ui64, ui64 + (1 << (64 - precision + 2))))\n",
    "            if precision % 2 == 0:\n",
    "                # lat,lon = (0, 0) and even precision\n",
    "                ui64 = _uint64_interleave(lat, lon - lon_grid)\n",
    "                ranges.append((ui64, ui64 + (1 << (64 - precision + 1))))\n",
    "\n",
    "                if lat > 0:\n",
    "                    ui64 = _uint64_interleave(lat - lat_grid, lon - lon_grid)\n",
    "                    ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "                    ui64 = _uint64_interleave(lat - lat_grid, lon)\n",
    "                    ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "                    ui64 = _uint64_interleave(lat - lat_grid, lon + lon_grid)\n",
    "                    ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "            else:\n",
    "                # lat,lon = (0, 0) and odd precision\n",
    "                if lat > 0:\n",
    "                    ui64 = _uint64_interleave(lat - lat_grid, lon)\n",
    "                    ranges.append((ui64, ui64 + (1 << (64 - precision + 1))))\n",
    "\n",
    "                    ui64 = _uint64_interleave(lat - lat_grid, lon - lon_grid)\n",
    "                    ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "                ui64 = _uint64_interleave(lat, lon - lon_grid)\n",
    "                ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "                ui64 = _uint64_interleave(lat + lat_grid, lon - lon_grid)\n",
    "                ranges.append((ui64, ui64 + (1 << (64 - precision))))\n",
    "\n",
    "    ranges.sort()\n",
    "\n",
    "    # merge the conditions\n",
    "    shrink = []\n",
    "    prev = None\n",
    "    for i in ranges:\n",
    "        if prev:\n",
    "            if prev[1] != i[0]:\n",
    "                shrink.append(prev)\n",
    "                prev = i\n",
    "            else:\n",
    "                prev = (prev[0], i[1])\n",
    "        else:\n",
    "            prev = i\n",
    "\n",
    "    shrink.append(prev)\n",
    "\n",
    "    ranges = []\n",
    "    for i in shrink:\n",
    "        a, b = i\n",
    "        if a == 0:\n",
    "            a = None  # we can remove the condition because it is the lowest value\n",
    "        if b == 0x10000000000000000:\n",
    "            b = None  # we can remove the condition because it is the highest value\n",
    "\n",
    "        ranges.append((a, b))\n",
    "\n",
    "    return ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
